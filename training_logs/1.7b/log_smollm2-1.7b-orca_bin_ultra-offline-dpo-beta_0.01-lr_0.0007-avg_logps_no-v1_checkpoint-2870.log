/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py:843: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  r = torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
INFO:root:Using nproc_per_node=4.
/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
[2025-05-10 17:28:40,505] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[2025-05-10 17:28:40,506] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[2025-05-10 17:28:40,506] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[2025-05-10 17:28:40,506] [WARNING] [real_accelerator.py:174:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.
[2025-05-10 17:28:40,506] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[2025-05-10 17:28:40,506] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[2025-05-10 17:28:40,506] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
[2025-05-10 17:28:40,507] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cpu (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/saeednjf/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/saeednjf/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/saeednjf/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/saeednjf/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__main__.py", line 10, in <module>
    from lm_eval import evaluator, utils
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/evaluator.py", line 15, in <module>
    import lm_eval.models
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/__init__.py", line 1, in <module>
    from . import (
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/vllm_causallms.py", line 28, in <module>
    from vllm import LLM, SamplingParams
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 11, in <module>
    from vllm.config import (CacheConfig, ConfigFormat, DecodingConfig,
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/config.py", line 12, in <module>
    from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/__init__.py", line 1, in <module>
    from vllm.model_executor.parameter import (BasevLLMParameter,
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/parameter.py", line 7, in <module>
    from vllm.distributed import get_tensor_model_parallel_rank
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/__init__.py", line 1, in <module>
    from .communication_op import *
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/communication_op.py", line 6, in <module>
    from .parallel_state import get_tp_group
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 38, in <module>
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__main__.py", line 10, in <module>
    from vllm.platforms import current_platform
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/__init__.py", line 67, in <module>
    from lm_eval import evaluator, utils
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .cuda import CudaPlatform
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 86, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/evaluator.py", line 15, in <module>
    import lm_eval.models
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/__init__.py", line 1, in <module>
    warn_if_different_devices()
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
    from . import (
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/vllm_causallms.py", line 28, in <module>
    return fn(*args, **kwargs)
           ^^^^^^^    ^from vllm import LLM, SamplingParams^
^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in warn_if_different_devices
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
   File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 11, in <module>
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in <listcomp>
    from vllm.config import (CacheConfig, ConfigFormat, DecodingConfig,
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/config.py", line 12, in <module>
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^    
from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
Traceback (most recent call last):
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/__init__.py", line 1, in <module>
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__main__.py", line 10, in <module>
    return fn(*args, **kwargs)
               ^from vllm.model_executor.parameter import (BasevLLMParameter,^
^^^  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/parameter.py", line 7, in <module>
^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 58, in get_physical_device_name
    from vllm.distributed import get_tensor_model_parallel_rank
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/__init__.py", line 1, in <module>
    from lm_eval import evaluator, utils
    handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
       File "/home/saeednjf/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 2604, in nvmlDeviceGetHandleByIndex
    from .communication_op import *
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/communication_op.py", line 6, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/evaluator.py", line 15, in <module>
    from .parallel_state import get_tp_group
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 38, in <module>
    import lm_eval.models
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/__init__.py", line 1, in <module>
    from vllm.platforms import current_platform
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/__init__.py", line 67, in <module>
    from . import (
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/vllm_causallms.py", line 28, in <module>
    from .cuda import CudaPlatform
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 86, in <module>
    from vllm import LLM, SamplingParams
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    _nvmlCheckReturn(ret)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 1042, in _nvmlCheckReturn
    warn_if_different_devices()
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 11, in <module>
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^    
from vllm.config import (CacheConfig, ConfigFormat, DecodingConfig,
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in warn_if_different_devices
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/config.py", line 12, in <module>
    raise NVMLError(ret)
pynvml.NVMLError_Unknown: Unknown Error
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
    from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS
     File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/__init__.py", line 1, in <module>
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in <listcomp>
    from vllm.model_executor.parameter import (BasevLLMParameter,
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/parameter.py", line 7, in <module>
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
            from vllm.distributed import get_tensor_model_parallel_rank 
           File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/__init__.py", line 1, in <module>
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
    from .communication_op import *
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/communication_op.py", line 6, in <module>
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 58, in get_physical_device_name
    from .parallel_state import get_tp_group
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 38, in <module>
    handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
             ^^^^^^^^^^^^^^^^^^    ^from vllm.platforms import current_platform^
^^^^^^^^^^^^  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/__init__.py", line 67, in <module>
^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 2604, in nvmlDeviceGetHandleByIndex
    from .cuda import CudaPlatform
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 86, in <module>
    warn_if_different_devices()
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^Traceback (most recent call last):
^^^^^^  File "<frozen runpy>", line 198, in _run_module_as_main
^^^^  File "<frozen runpy>", line 88, in _run_code
^^^  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__main__.py", line 10, in <module>

  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in warn_if_different_devices
    _nvmlCheckReturn(ret)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 1042, in _nvmlCheckReturn
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
                   ^^^^^^^^^^^^^^^^^^^^    ^from lm_eval import evaluator, utils^
^^^^^^^^^^^^^^^^^^^  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in <listcomp>
    raise NVMLError(ret)
pynvml.NVMLError_Unknown: Unknown Error
    from .evaluator import evaluate, simple_evaluate
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/evaluator.py", line 15, in <module>
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
    import lm_eval.models
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/__init__.py", line 1, in <module>
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 58, in get_physical_device_name
    from . import (
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/vllm_causallms.py", line 28, in <module>
    handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^from vllm import LLM, SamplingParams^
^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 2604, in nvmlDeviceGetHandleByIndex
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 11, in <module>
    from vllm.config import (CacheConfig, ConfigFormat, DecodingConfig,
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/config.py", line 12, in <module>
    _nvmlCheckReturn(ret)
    from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 1042, in _nvmlCheckReturn
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/__init__.py", line 1, in <module>
    from vllm.model_executor.parameter import (BasevLLMParameter,
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/model_executor/parameter.py", line 7, in <module>
    raise NVMLError(ret)
    from vllm.distributed import get_tensor_model_parallel_rank
pynvml.NVMLError_Unknown  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/__init__.py", line 1, in <module>
: Unknown Error
    from .communication_op import *
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/communication_op.py", line 6, in <module>
    from .parallel_state import get_tp_group
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/distributed/parallel_state.py", line 38, in <module>
    from vllm.platforms import current_platform
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/__init__.py", line 67, in <module>
    from .cuda import CudaPlatform
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 86, in <module>
    warn_if_different_devices()
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in warn_if_different_devices
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 73, in <listcomp>
    device_names = [get_physical_device_name(i) for i in range(device_ids)]
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 41, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/vllm/platforms/cuda.py", line 58, in get_physical_device_name
    handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 2604, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/pynvml.py", line 1042, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.NVMLError_Unknown: Unknown Error
W0510 17:28:42.514000 130261396617024 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1340497 closing signal SIGTERM
W0510 17:28:42.515000 130261396617024 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1340499 closing signal SIGTERM
W0510 17:28:42.515000 130261396617024 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1340500 closing signal SIGTERM
E0510 17:28:42.579000 130261396617024 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1340498) of binary: /home/saeednjf/miniconda3/envs/llm-env/bin/python
Traceback (most recent call last):
  File "/home/saeednjf/miniconda3/envs/llm-env/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1204, in launch_command
    multi_gpu_launcher(args)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/accelerate/commands/launch.py", line 825, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
lm_eval FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-10_17:28:42
  host      : rack02-06
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1340498)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
