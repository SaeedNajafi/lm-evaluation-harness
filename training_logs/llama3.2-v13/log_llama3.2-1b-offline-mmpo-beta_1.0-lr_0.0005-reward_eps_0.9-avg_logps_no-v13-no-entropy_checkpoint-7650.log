The following values were not passed to `accelerate launch` and had defaults used instead:
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
2025-08-21:13:49:44 INFO     [__main__:428] Passed `--trust_remote_code`, setting environment variable `HF_DATASETS_TRUST_REMOTE_CODE=true`
2025-08-21:13:49:44 INFO     [__main__:440] Selected Tasks: ['arc_challenge', 'arc_easy', 'commonsense_qa', 'hellaswag', 'mathqa', 'mmlu', 'openbookqa', 'piqa', 'race', 'winogrande']
2025-08-21:13:49:44 INFO     [evaluator:185] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-08-21:13:49:44 WARNING  [evaluator:197] generation_kwargs: {'max_new_tokens': 1024, 'do_sample': False} specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!
2025-08-21:13:49:44 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/home/saeednjf/projects/def-afyshe-ab/saeednjf/checkpoints/llama-3.2-1b-mmpo-v13/llama3.2-1b-offline-mmpo-beta_1.0-lr_0.0005-reward_eps_0.9-avg_logps_no-v13-no-entropy/checkpoint-7650', 'dtype': 'bfloat16', 'trust_remote_code': True}
2025-08-21:13:49:44 INFO     [models.huggingface:137] Using device 'cuda'
2025-08-21:13:49:45 INFO     [models.huggingface:382] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/home/saeednjf/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 600, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 316, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5061, in from_pretrained
    ) = cls._load_pretrained_model(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5482, in _load_pretrained_model
    caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/transformers/modeling_utils.py", line 6100, in caching_allocator_warmup
    index = device.index if device.index is not None else torch_accelerator_module.current_device()
                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py", line 878, in current_device
    _lazy_init()
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/torch/cuda/__init__.py", line 314, in _lazy_init
    torch._C._cuda_init()
RuntimeError: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.
Traceback (most recent call last):
  File "/home/saeednjf/miniconda3/envs/llm-env/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/home/saeednjf/miniconda3/envs/llm-env/lib/python3.11/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/saeednjf/miniconda3/envs/llm-env/bin/python', 'lm_eval', '--model', 'hf', '--apply_chat_template', '--write_out', '--model_args', 'pretrained=/home/saeednjf/projects/def-afyshe-ab/saeednjf/checkpoints/llama-3.2-1b-mmpo-v13/llama3.2-1b-offline-mmpo-beta_1.0-lr_0.0005-reward_eps_0.9-avg_logps_no-v13-no-entropy/checkpoint-7650,dtype=bfloat16', '--tasks', 'winogrande,arc_easy,piqa,hellaswag,openbookqa,arc_challenge,mmlu,mathqa,race,commonsense_qa', '--batch_size', '4', '--num_fewshot', '0', '--seed', '42', '--trust_remote_code', '--gen_kwargs', 'max_new_tokens=1024,do_sample=False', '--output_path', 'training_logs/lm_harness_output_llama3.2-1b-offline-mmpo-beta_1.0-lr_0.0005-reward_eps_0.9-avg_logps_no-v13-no-entropy_checkpoint-7650']' returned non-zero exit status 1.
