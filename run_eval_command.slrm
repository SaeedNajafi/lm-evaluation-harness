#!/bin/bash

#SBATCH --job-name=lm-harness
#SBATCH --account=def-afyshe-ab
#SBATCH --nodes=1
#SBATCH --gpus-per-node=1
#SBATCH --cpus-per-gpu=4
#SBATCH --mem=64G
#SBATCH --time=02:00:00
#SBATCH --open-mode=append
#SBATCH --wait-all-nodes=1
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

module --force purge

eval "$(conda shell.bash hook)"
conda activate llm-env

# For reading key=value arguments
for ARGUMENT in "$@"
do
	KEY=$(echo $ARGUMENT | cut -f1 -d=)
	KEY_LENGTH=${#KEY}
	VALUE="${ARGUMENT:$KEY_LENGTH+1}"
	export "$KEY"="$VALUE"
done

export CUDA_HOME=$CONDA_PREFIX
export NCCL_HOME=$CONDA_PREFIX
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib

date;pwd

export MASTER_ADDR="$(hostname --fqdn)"
export MASTER_PORT="$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1])')"
export RDVZ_ID=$RANDOM
echo "RDZV Endpoint $MASTER_ADDR:$MASTER_PORT"


export WANDB_PROJECT="mmpo-project-lm-harness"
export WANDB_MODE="offline"
export ACCELERATE_LOG_LEVEL="info"
export HF_ENDPOINT="https://huggingface.co"

LOG_DIR="training_logs"
LOG_PATH="${LOG_DIR}/log_${NAME}.log"
# Make logging directories.
mkdir -p "${LOG_DIR}"
echo "Placing logs in: ${LOG_DIR}"

NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

NUM_PROCS=$((NUM_GPUs*SLURM_NNODES))

srun -l \
    bash -c "accelerate launch \
    --num_machines=1 \
    --num_processes=$NUM_PROCS \
    --machine_rank=0 \
    --main_process_ip=$MASTER_ADDR \
    --main_process_port=$MASTER_PORT \
    --rdzv_backend=static \
    lm_eval --model hf \
    --apply_chat_template \
    --write_out \
    --model_args pretrained=${MODEL_PATH},dtype=bfloat16 \
    --tasks winogrande,arc_easy,piqa,hellaswag,openbookqa,arc_challenge,mmlu,mathqa,race,commonsense_qa \
    --batch_size 4 \
    --num_fewshot 0 \
    --seed 42 \
    --trust_remote_code \
    --gen_kwargs max_new_tokens=1024,do_sample=False \
    --output_path training_logs/lm_harness_output_${NAME} > ${LOG_PATH} 2>&1"